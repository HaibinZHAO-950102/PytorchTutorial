{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3c8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a034c8d",
   "metadata": {},
   "source": [
    "# 尽量不要自己给自己赋值\n",
    "## 错误例子：自己给自己赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86946be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0312, 0.6983, 0.4285, 0.4792, 0.2871, 0.7417, 0.1106, 0.9623, 0.9452,\n",
       "        0.4848], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta1 = torch.nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "eta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23673e",
   "metadata": {},
   "source": [
    "这里本来想让`eta1`乘以2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ea4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta1 = eta1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67657095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0625, 1.3966, 0.8570, 0.9584, 0.5741, 1.4835, 0.2212, 1.9247, 1.8905,\n",
       "        0.9695], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta1.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ee839",
   "metadata": {},
   "source": [
    "结果可以看出没有梯度了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a9bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haibinzhao/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646755922314/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "eta1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a1a12",
   "metadata": {},
   "source": [
    "## 正确例子：赋值给其他变量名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d74691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.8627, 0.4348, 0.9095, 0.5124, 0.3881, 0.6686, 0.7270, 0.6214, 0.1460,\n",
       "        0.6846], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta2 = torch.nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "eta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5a96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_temp = eta2 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66db1861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7255, 0.8697, 1.8190, 1.0247, 0.7762, 1.3372, 1.4540, 1.2428, 0.2920,\n",
       "        1.3692], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b658656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_temp.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821be213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da59f5",
   "metadata": {},
   "source": [
    "# 小心直接赋值其他变量\n",
    "## 错误例子：直接赋值给别的变量后，内存里还是同一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e820e6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2928, 0.7851, 0.0654, 0.9038, 0.8469, 0.8558, 0.8001, 0.8354, 0.2006,\n",
       "        0.9795], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921ad80",
   "metadata": {},
   "source": [
    "把`v1`赋值给`a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49506fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = v1[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389e619",
   "metadata": {},
   "source": [
    "修改`a`的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e84a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1814, 0.2235, 0.1048, 0.1320], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data.copy_(torch.rand(4))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb488f2",
   "metadata": {},
   "source": [
    "发现`v1`的值也被改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d73cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1814, 0.2235, 0.1048, 0.1320, 0.8469, 0.8558, 0.8001, 0.8354, 0.2006,\n",
       "        0.9795], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97ff55",
   "metadata": {},
   "source": [
    "## 正确例子：用克隆的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763f1b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6647, 0.6194, 0.3355, 0.2664, 0.4136, 0.1148, 0.5026, 0.8328, 0.2710,\n",
       "        0.9257], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = torch.nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc257b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = v2[:4].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cfc3cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3222, 0.6089, 0.8604, 0.0538], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.data.copy_(torch.rand(4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eee5631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6647, 0.6194, 0.3355, 0.2664, 0.4136, 0.1148, 0.5026, 0.8328, 0.2710,\n",
       "        0.9257], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79c321",
   "metadata": {},
   "source": [
    "# 生成可学习参数时先生成，最后再包装\n",
    "## 错误例子：先生成Parameter，再变形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "185f638f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5475],\n",
       "        [ 2.0614],\n",
       "        [-0.0118],\n",
       "        [ 1.6362],\n",
       "        [ 0.0855],\n",
       "        [ 0.2819],\n",
       "        [-1.6987],\n",
       "        [-1.1044],\n",
       "        [ 1.1377],\n",
       "        [ 0.4085],\n",
       "        [ 1.4628],\n",
       "        [-0.2824]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.nn.Parameter(torch.randn(12,1), requires_grad=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9672158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5475,  2.0614, -0.0118,  1.6362],\n",
       "        [ 0.0855,  0.2819, -1.6987, -1.1044],\n",
       "        [ 1.1377,  0.4085,  1.4628, -0.2824]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = t.view(3,4)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c9cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a27b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haibinzhao/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646755922314/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de9377",
   "metadata": {},
   "source": [
    "## 正确例子：先全部初始化完成，再包装成Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c23a63e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2693],\n",
       "        [ 0.2612],\n",
       "        [-0.8253],\n",
       "        [ 0.5455],\n",
       "        [ 1.5656],\n",
       "        [-1.4120],\n",
       "        [ 1.5922],\n",
       "        [-1.7210],\n",
       "        [ 0.8812],\n",
       "        [ 0.9397],\n",
       "        [-0.9860],\n",
       "        [ 1.3318]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(12,1)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31fcac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k.view(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8f9cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = torch.nn.Parameter(k, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4509f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db7cdd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bb60e",
   "metadata": {},
   "source": [
    "# 组装变量时的错误\n",
    "## 错误例子：用tensor把变量重新拼起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fdd1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter(torch.rand(5), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95e3dfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1184, grad_fn=<MulBackward0>),\n",
       " tensor(0.3061, grad_fn=<LogBackward0>),\n",
       " tensor(0.0003, grad_fn=<PowBackward0>),\n",
       " tensor(1.3874, grad_fn=<SoftplusBackward0>),\n",
       " tensor(0.5207, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = a[0] * 2\n",
    "a1 = torch.log(a[1]+1)\n",
    "a2 = a[2] ** 2\n",
    "a3 = torch.nn.functional.softplus(a[3] * 2)\n",
    "a4 = a[4]\n",
    "a0, a1, a2, a3, a4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0d655",
   "metadata": {},
   "source": [
    "组合成一个新变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8e1a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1843e-01, 3.0607e-01, 2.5946e-04, 1.3874e+00, 5.2072e-01])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = torch.tensor([a0, a1, a2, a3, a4])\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e5ced",
   "metadata": {},
   "source": [
    "由于重新用`tensor`包装了变量，切断了反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f4faf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mA1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "A1.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea05e5",
   "metadata": {},
   "source": [
    "## 正确例子：建立一个变量，然后把值填进去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa4f144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0592, 0.3581, 0.0161, 0.5500, 0.5207], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fdb7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = torch.zeros([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "818b4ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1843e-01, 3.0607e-01, 2.5946e-04, 1.3874e+00, 5.2072e-01],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2[0] = a[0] * 2\n",
    "A2[1] = torch.log(a[1]+1)\n",
    "A2[2] = a[2] ** 2\n",
    "A2[3] = torch.nn.functional.softplus(a[3] * 2)\n",
    "A2[4] = a[4]\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af41fa5",
   "metadata": {},
   "source": [
    "就可以使用了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9171f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9c826d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 0.7363, 0.0322, 1.5006, 1.0000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426776df",
   "metadata": {},
   "source": [
    "## 正确例子2： 用cat或stack拼起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "173d9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = torch.stack((a[0]*2,\n",
    "                torch.log(a[1]+1),\n",
    "                a[2]**2,\n",
    "                torch.nn.functional.softplus(a[3]*2),\n",
    "                a[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7472ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad.zero_()\n",
    "A3.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f77f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 0.7363, 0.0322, 1.5006, 1.0000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcaee32",
   "metadata": {},
   "source": [
    "# 可学习参数有相互组合时，不能填进去，只能cat/stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485f07d",
   "metadata": {},
   "source": [
    "在某些情况下，神经网络的可学习参数需要经过一些变化，然后继续进行计算。例如在某些问题中，可学习参数$R_1$, $R_2$需要经过一个黑箱过程才能变成$\\eta$，然后$\\eta$会参与后续的运算。也就是\n",
    "$$\n",
    "[R_1,R_2]\\to Black\\ Box \\to \\eta \\to Neural Network\n",
    "$$\n",
    "\n",
    "这个黑箱转换过程可以通过神经网络来实现，也就是\n",
    "$$\n",
    "[R_1,R_2]\\to NN \\to \\eta \\to Neural Network\n",
    "$$\n",
    "有的时候$R_1$和$R_2$的比值是一个重要的因素。但是这个比值会随着normalization而被削弱。在这种情况下，人们往往人为添加一个特征，也就是训练一个3输入1输出的$NN$来拟合黑箱，也就是输入变成$R_1, R_2, k$，其中$k=\\frac{R_1}{R_2}$。然后整个输入会被normalize成为$R_1^N, R_2^N, k^N$，注意$k^N\\ne \\frac{R_1^N}{R_2^N}$。\n",
    "\n",
    "然而，我们明显能看出$R_1^N, R_2^N, k^N$只有2个自由度，也就是可学习参数最多只能有2个，否则就会产生矛盾。那么比如我们让可学习参数为$R_1^N$和$R_2^N$。然后推算出$k^N=\\mathcal{N}\\left\\{\\frac{\\mathcal{D}(R_1^N)}{\\mathcal{D}(R_2^N)}\\right\\}$，其中$\\mathcal{N}(\\cdot)$和$\\mathcal{D}(\\cdot)$表示normalization和denormalization的操作。然后把$R_1^N$和$R_2^N$和推导出来的$k^N$拼起来，放到$NN$里面计算$\\eta$。然而，这个过程会出现问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "038f5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class Example(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # learnable R1n and R2n\n",
    "        self.paramn_ = torch.nn.Parameter(torch.rand(2),requires_grad=True) \n",
    "        # exemplary NN convert [R1n, R2n, kn] to eta\n",
    "        self.eta_estimator = torch.nn.Sequential(torch.nn.Linear(3,1))\n",
    "        self.eta_estimator.train(False)\n",
    "        # exemplary max and min of [R1, R2, k] for normalization and denormalization\n",
    "        self.param_max = torch.tensor([2, 4, 2])\n",
    "        self.param_min = torch.tensor([1, 2, 0])\n",
    "\n",
    "    @property\n",
    "    def Param(self):\n",
    "        # calculate normalized R1\n",
    "        paramn = torch.zeros([3])\n",
    "        paramn[0] = self.paramn_[0]    # R1n\n",
    "        paramn[1] = self.paramn_[1]    # R2n\n",
    "        # denormalization\n",
    "        param = paramn * (self.param_max - self.param_min) + self.param_min\n",
    "        # calculate k=R1/R2\n",
    "        param[2] = param[0] / param[1]\n",
    "        # normalization\n",
    "        paramn = (param - self.param_min) / (self.param_max - self.param_min)\n",
    "        return paramn\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return self.eta_estimator(self.Param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec34fd",
   "metadata": {},
   "source": [
    "可以看出上述代码以$R_1^N$和$R_2^N$为可学习参数，然后先经过denormalization求出真实值$R_1$和$R_2$，然后在利用真实值求出$k$，再把真实的$[R_1, R_2,k]$进行normalization，就可以用于放到$NN$里用于计算$\\eta$了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed53a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可学习参数：\n",
      "Parameter containing:\n",
      "tensor([0.4963, 0.7682], requires_grad=True)\n",
      "经过处理后的可学习参数\n",
      "tensor([0.4963, 0.7682, 0.2115], grad_fn=<DivBackward0>)\n",
      "利用可学习参数转换成的eta\n",
      "tensor([-0.4544], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = Example()\n",
    "print('可学习参数：')\n",
    "print(test.paramn_)\n",
    "print('经过处理后的可学习参数')\n",
    "print(test.Param)\n",
    "print('利用可学习参数转换成的eta')\n",
    "print(test.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df3dc5",
   "metadata": {},
   "source": [
    "然而，由于`paramn`里的第三个参数$k$是前两个的组合，反向传递时会出现问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c24c7c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "test.Param.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b729d75",
   "metadata": {},
   "source": [
    "问题就在于`param`的第三个参数$k$是用`param[2]=param[0]/param[1]`填进去的。应该用cat（高维度）或者stack（0维）拼起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd8913",
   "metadata": {},
   "source": [
    "## 正确例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "532c3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class Example(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # learnable R1n and R2n\n",
    "        self.paramn_ = torch.nn.Parameter(torch.rand(2),requires_grad=True) \n",
    "        # exemplary NN convert [R1n, R2n, kn] to eta\n",
    "        self.eta_estimator = torch.nn.Sequential(torch.nn.Linear(3,1))\n",
    "        self.eta_estimator.train(False)\n",
    "        # exemplary max and min of [R1, R2, k] for normalization and denormalization\n",
    "        self.param_max = torch.tensor([2, 4, 2])\n",
    "        self.param_min = torch.tensor([1, 2, 0])\n",
    "\n",
    "    @property\n",
    "    def Param(self):\n",
    "        # calculate normalized R1\n",
    "        paramn = torch.zeros([3])\n",
    "        paramn[0] = self.paramn_[0]    # R1n\n",
    "        paramn[1] = self.paramn_[1]    # R2n\n",
    "        # denormalization\n",
    "        param = paramn * (self.param_max - self.param_min) + self.param_min\n",
    "        # calculate k=R1/R2\n",
    "        param_temp = torch.stack([param[0], param[1], param[0]/param[1]])\n",
    "        # normalization\n",
    "        paramn = (param_temp - self.param_min) / (self.param_max - self.param_min)\n",
    "        return paramn\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return self.eta_estimator(self.Param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c71551a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可学习参数：\n",
      "Parameter containing:\n",
      "tensor([0.4963, 0.7682], requires_grad=True)\n",
      "经过处理后的可学习参数\n",
      "tensor([0.4963, 0.7682, 0.2115], grad_fn=<DivBackward0>)\n",
      "利用可学习参数转换成的eta\n",
      "tensor([-0.4544], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = Example()\n",
    "print('可学习参数：')\n",
    "print(test.paramn_)\n",
    "print('经过处理后的可学习参数')\n",
    "print(test.Param)\n",
    "print('利用可学习参数转换成的eta')\n",
    "print(test.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "086da82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1414, 0.8804])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Param.sum().backward()\n",
    "test.paramn_.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ff0d4",
   "metadata": {},
   "source": [
    "## 其他解决方案\n",
    "由于关键原因在于`paramn`的第三个元素是前两个元素的组合，这会使PyTorch出现问题。我们可以采取另外的解决方案：既然只有2个自由度可以学习，可以将任意一个变量detach掉，这样也可以解决此问题，但是会牺牲掉detach掉的分支的信息，因此不推荐："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94a8a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class Example(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # learnable R1n and R2n\n",
    "        self.paramn_ = torch.nn.Parameter(torch.rand(2),requires_grad=True) \n",
    "        # exemplary NN convert [R1n, R2n, kn] to eta\n",
    "        self.eta_estimator = torch.nn.Sequential(torch.nn.Linear(3,1))\n",
    "        self.eta_estimator.train(False)\n",
    "        # exemplary max and min of [R1, R2, k] for normalization and denormalization\n",
    "        self.param_max = torch.tensor([2, 4, 2])\n",
    "        self.param_min = torch.tensor([1, 2, 0])\n",
    "\n",
    "    @property\n",
    "    def Param(self):\n",
    "        # calculate normalized R1\n",
    "        paramn = torch.zeros([3])\n",
    "        paramn[0] = self.paramn_[0]    # R1n\n",
    "        paramn[1] = self.paramn_[1]    # R2n\n",
    "        # denormalization\n",
    "        param = paramn * (self.param_max - self.param_min) + self.param_min\n",
    "        # calculate k=R1/R2\n",
    "        param[2] = (param[0] / param[1]).detach()\n",
    "        # normalization\n",
    "        paramn = (param - self.param_min) / (self.param_max - self.param_min)\n",
    "        return paramn\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return self.eta_estimator(self.Param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "033bd2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可学习参数：\n",
      "Parameter containing:\n",
      "tensor([0.4963, 0.7682], requires_grad=True)\n",
      "经过处理后的可学习参数\n",
      "tensor([0.4963, 0.7682, 0.2115], grad_fn=<DivBackward0>)\n",
      "利用可学习参数转换成的eta\n",
      "tensor([-0.4544], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = Example()\n",
    "print('可学习参数：')\n",
    "print(test.paramn_)\n",
    "print('经过处理后的可学习参数')\n",
    "print(test.Param)\n",
    "print('利用可学习参数转换成的eta')\n",
    "print(test.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c65780b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Param.sum().backward()\n",
    "test.paramn_.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a7cc8",
   "metadata": {},
   "source": [
    "## 总结\n",
    "可以看出3个小实验的forward的结果都是一样的，但是第一个实验无法反向传播，后面两个可以。但是相比于第二个，第三个会把detach掉的分支的信息给忽略，这不是最优的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047dcc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7383e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aea961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619019a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3734de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca747db-c9ac-4894-b602-89513135b969",
   "metadata": {},
   "source": [
    "# Package Installation\n",
    "1. Pytorch\n",
    "\n",
    "    For `conda` user:\n",
    "    \n",
    "    `!conda install pytorch==1.10.0 torchvision==0.11.0 torchaudio==0.10.0 -c pytorch` or \n",
    "\n",
    "    For `pip` user:\n",
    "\n",
    "    `!pip3 install torch torchvision torchaudio`\n",
    "\n",
    "    For other choice (e.g., with/without GPU, different OS), please see https://pytorch.org/.\n",
    "    \n",
    "    *Note that to use SDIL GPU, a higher version must be installed, whatever `conda` or `pip`*\n",
    "    \n",
    "    `pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html`\n",
    "\n",
    "2. Packages for GPU-checking\n",
    "    \n",
    "    For `conda` user:\n",
    "\n",
    "    `!conda install -c conda-forge psutil`\n",
    "    \n",
    "    `!conda install -c conda-forge humanize`\n",
    "    \n",
    "    `!conda install -c conda-forge gputil`\n",
    "\n",
    "    For `pip` user:\n",
    "\n",
    "    `!pip3 psutil`\n",
    "    \n",
    "    `!pip3 install humanize`\n",
    "    \n",
    "    `!pip3 install GPUtil`\n",
    "\n",
    "3. Packages for others\n",
    "\n",
    "    For `conda` user:\n",
    "\n",
    "    `!conda install calender`\n",
    "\n",
    "    For `pip` user:\n",
    "\n",
    "    `!pip3 install calender`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e059efa-ed11-43a7-9365-417b87cdf71e",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbd9e75-54c6-47a2-a8a6-53b72526c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "# for GPU checking\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "import matplotlib.pyplot as plt\n",
    "# for training NN\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118a8c7-5edf-41b7-a61c-012f5502c1e1",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbabe06-7e12-472a-8e76-2162a2de54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for the reproducibility\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28545bd7-22ff-4bca-a4b0-3accfb2e0d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 387.6 GB  | Proc size: 1.1 GB\n",
      "GPU RAM Free: 40314MB | Used: 39MB | Util   0% | Total 40960MB\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a87e14-0fd7-438a-ae59-d67c2a650c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select GPU if available, otherwise, CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f1c6a-c650-44a5-a6ab-3c5c256f3773",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5349a-844b-490d-a0eb-bb07359405f1",
   "metadata": {},
   "source": [
    "## Define Data-Preprocessing\n",
    "We apply easily the torchvision.transform to do the pre-processing. For further information, see https://pytorch.org/vision/stable/transforms.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46a1d7-1624-458d-a334-eaf6fb7406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize([0.5], [0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b334e8f-b033-48c5-92b1-ae8d3b2dddbf",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "1. create a folder to store data\n",
    "\n",
    "2. download data. If the dataset has been already downloaded once, `download` could be `False`, staying `True` is also no problem.\n",
    "\n",
    "3. apply the `transform` on dataset\n",
    "\n",
    "Note that, their might be a warning about \"non-writable NumPy Arrray\". To solve this warning, please go to the path for conda:\n",
    "\n",
    "Anaconda (or Miniconda)/envs/MyEnvironment/lib/python3.8/site-packages/torchvision/datasets/mnist.py\n",
    "\n",
    "in line 498 (might changes slightly from version to version), delete `copy=False`, i.e., from\n",
    "\n",
    "`return torch.from_numpy(parsed.astype(m[2]), copy=False).view(*s)` to\n",
    "\n",
    "`return torch.from_numpy(parsed.astype(m[2])).view(*s)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a477bd-6048-4833-a495-95c4b58faf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c5afa-e8d7-4884-bc2c-db9a11165c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_origin = torchvision.datasets.FashionMNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "train_data_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ffbdc-4b22-444e-b171-40ea9b60b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data into train and valid data\n",
    "N_train = len(train_data_origin)\n",
    "N_train_new = int(N_train * 0.8)\n",
    "N_valid_new = N_train - N_train_new\n",
    "N_train_new, N_valid_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae1c9e-0060-4261-b953-366019c597da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = torch.utils.data.random_split(train_data_origin, [N_train_new, N_valid_new])\n",
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd27a4c-c667-45ee-a441-8b6633b7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.FashionMNIST(root='./data/', train=False, download=True, transform=transform)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e99d3-ed0f-4966-a7ec-cd55a0f0fccb",
   "metadata": {},
   "source": [
    "## Build dataloader\n",
    "Details for loader see https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2833429-17b9-4e84-9eab-2ad735ae75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7e373-55b7-4c4e-9748-55855a4363d3",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44fed3-f5e1-4ee6-ab3b-62062ea90ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = iter(train_loader)\n",
    "img_temp, label_temp = data_temp.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ece8d-66d6-4e39-b361-781882737b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(5, 10, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(img_temp[i]), cmap='gray')\n",
    "    ax.set_title(label_temp[i].item())\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6a6ba-57f4-403d-82b4-7ff210337977",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6a480-3740-4793-8b6b-87fd82a8984e",
   "metadata": {},
   "source": [
    "## Define Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437fb46-d346-4170-9eed-c41930c7580a",
   "metadata": {},
   "source": [
    "### Check in-/output dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c58e5c-fccf-4a14-9772-527902c1ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb578c-84db-4cc4-92fd-f5da21e5131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_feature = np.prod(img_temp[0].shape)\n",
    "N_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e72a7e-52a1-40b0-8eb6-aa986a69c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c8abe-0f67-4664-99fd-d03e39c2fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(N_feature, 256), torch.nn.PReLU(),\n",
    "                            torch.nn.Linear(256, 128),       torch.nn.PReLU(),\n",
    "                            torch.nn.Linear(128, 64),        torch.nn.PReLU(),\n",
    "                            torch.nn.Linear(64, N_class),    torch.nn.LogSoftmax(dim=1))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13a9cc-4aed-4fb5-8a50-ba12b982fa09",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3182c7-cdd8-40b7-a5f3-59e6580e050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "Lossfunction = torch.nn.CrossEntropyLoss()\n",
    "Lossfunction.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "Optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b73cc-cbd4-4a50-b203-cd01509db952",
   "metadata": {},
   "source": [
    "## Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e309b-40b7-40fb-8453-8726cc80e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn, train_loader, valid_loader, optimizer, lossfunction, Epoch=10**10):\n",
    "    # dir for save temporary files\n",
    "    if not os.path.exists('./temp'):\n",
    "        os.mkdir('./temp')\n",
    "    \n",
    "    # create an unique ID for saving temp file, avoiding file overwriting while multiple training\n",
    "    training_ID = ts = int(calendar.timegm(time.gmtime()))\n",
    "    print(f'The ID for this training is {training_ID}.')\n",
    "    \n",
    "    # initialize best valid loss for saving the best model\n",
    "    best_valid_loss = 10 ** 10\n",
    "    \n",
    "    # arrays to save training process\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    # to count the epoch without any improvement, for early stop\n",
    "    not_imporved = 0\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(Epoch):\n",
    "        # timer\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # some temp variables to calculate the loss and acc from mini-batch to batch\n",
    "        num_of_mini_batch  = []\n",
    "        loss_of_mini_batch = []\n",
    "        acc_of_mini_batch  = []\n",
    "        for X_train, y_train in train_loader:\n",
    "            # reshape 2D images into 1D, also transfer to device\n",
    "            X_train = X_train.view(X_train.shape[0], -1).to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            \n",
    "            # forward propagation\n",
    "            prediction_train = nn(X_train)\n",
    "            \n",
    "            # calculate loss\n",
    "            train_loss_mini_batch = lossfunction(prediction_train, y_train)\n",
    "\n",
    "            # calculate predicted class of input data\n",
    "            yhat_train = torch.argmax(prediction_train.data, 1)\n",
    "            \n",
    "            # calculate how many predictions are correct\n",
    "            train_correct = torch.sum(yhat_train == y_train.data)\n",
    "            \n",
    "            # calculate accuracy of prediction\n",
    "            train_acc_mini_batch = train_correct / y_train.numel()\n",
    "\n",
    "            # update parameters in model\n",
    "            optimizer.zero_grad()\n",
    "            train_loss_mini_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss and acc from mini-batch\n",
    "            num_of_mini_batch.append(X_train.shape[0])\n",
    "            loss_of_mini_batch.append(train_loss_mini_batch.item())\n",
    "            acc_of_mini_batch.append(train_acc_mini_batch.item())\n",
    "\n",
    "        # convert and record loss/acc from mini-batch to batch\n",
    "        train_loss = np.average(loss_of_mini_batch, weights=num_of_mini_batch)\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = np.average(acc_of_mini_batch, weights=num_of_mini_batch)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # similar as training, calculate loss and accuracy on valid data\n",
    "        num_of_mini_batch  = []\n",
    "        loss_of_mini_batch = []\n",
    "        acc_of_mini_batch  = []\n",
    "        with torch.no_grad():\n",
    "            for X_valid, y_valid in valid_loader:\n",
    "                X_valid = X_valid.view(X_valid.shape[0], -1).to(device)\n",
    "                y_valid = y_valid.to(device)\n",
    "                prediction_valid = nn(X_valid) \n",
    "                valid_loss_mini_batch = lossfunction(prediction_valid, y_valid).data\n",
    "                yhat_valid = torch.argmax(prediction_valid.data, 1)\n",
    "                valid_correct = torch.sum(yhat_valid == y_valid.data)\n",
    "                valid_acc_mini_batch = valid_correct / y_valid.numel()\n",
    "\n",
    "                num_of_mini_batch.append(X_valid.shape[0])\n",
    "                loss_of_mini_batch.append(valid_loss_mini_batch.item())\n",
    "                acc_of_mini_batch.append(valid_acc_mini_batch.item())\n",
    "\n",
    "            valid_loss = np.average(loss_of_mini_batch, weights=num_of_mini_batch)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_acc = np.average(acc_of_mini_batch, weights=num_of_mini_batch)\n",
    "            valid_accs.append(valid_acc)\n",
    "\n",
    "        # if valid loss in current epoch is better than previous one, save this model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(nn, f'./temp/NN_temp_{training_ID}')\n",
    "            random_state = torch.random.get_rng_state()\n",
    "            torch.save(random_state, f'./temp/NN_temp_random_state_{training_ID}')\n",
    "            not_imporved = 0\n",
    "        # if not, that means in this epoch, the model is not improved.\n",
    "        else:\n",
    "            not_imporved += 1\n",
    "            if not_imporved > 5:\n",
    "                print('Early stop.')\n",
    "                break\n",
    "        \n",
    "        # timer\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        # print information about current epoch\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'| Epoch: {epoch:-5d} | Train acc: {train_acc:-.5f} | Train loss: {train_loss:-.5e} | Valid acc: {valid_acc:-.5f} | Valid loss: {valid_loss:-.5e} | run time: {end_time-start_time} |')\n",
    "            \n",
    "    print('Finished.')\n",
    "    \n",
    "    return torch.load(f'./temp/NN_temp_{training_ID}'), train_losses, valid_losses, train_accs, valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024731e-30b6-476e-afa2-c6be35d307cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss, valid_loss, train_acc, valid_acc = train_nn(model, train_loader, valid_loader, Optimizer, Lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfbc9d-b090-4c81-ad89-31086b97f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='train loss');\n",
    "plt.plot(valid_loss, label='valid loss');\n",
    "plt.xlabel('epoch')\n",
    "plt.xlabel('loss')\n",
    "plt.title('Loss of training')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921bb10-baa1-422c-b29e-19309914181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, label='train acc');\n",
    "plt.plot(valid_acc, label='valid acc');\n",
    "plt.xlabel('epoch')\n",
    "plt.xlabel('accuracy')\n",
    "plt.title('Acc of training')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fb203-7609-4abb-9189-ee56a63afb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_mini_batch  = []\n",
    "loss_of_mini_batch = []\n",
    "acc_of_mini_batch  = []\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test = X_test.view(X_test.shape[0], -1).to(device)\n",
    "        y_test = y_test.to(device)\n",
    "        prediction_test = model(X_test) \n",
    "        test_loss_mini_batch = Lossfunction(prediction_test, y_test).data\n",
    "        yhat_test = torch.argmax(prediction_test.data, 1)\n",
    "        test_correct = torch.sum(yhat_test == y_test.data)\n",
    "        test_acc_mini_batch = test_correct / y_test.numel()\n",
    "\n",
    "        num_of_mini_batch.append(X_test.shape[0])\n",
    "        loss_of_mini_batch.append(test_loss_mini_batch.item())\n",
    "        acc_of_mini_batch.append(test_acc_mini_batch.item())\n",
    "\n",
    "    test_loss = np.average(loss_of_mini_batch, weights=num_of_mini_batch)\n",
    "    test_acc = np.average(acc_of_mini_batch, weights=num_of_mini_batch)\n",
    "\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d4dc4-5e50-4378-b5fb-57c99e071c5e",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66f4c9-b391-42a9-aabc-8f9d61eb2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./result'):\n",
    "        os.mkdir('./result')\n",
    "torch.save(model, './result/MyModel.nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e2525-7756-4858-a735-466dfe9c11b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971a65b-d994-466f-b9d7-b7d595c0e610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4ddea-1783-456a-891e-ae8ac2c6bec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5d968-4d10-406c-b86d-e8f9dec7f7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e74c6-f493-48ff-b9ec-fbd00f64061b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c605a89-4572-4413-86e6-4477faef4635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c4355-a90b-4dd2-ac8f-4b0b913823b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121754e4-67e3-44fc-8b5f-87a5996bc202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ceb162-d97f-4c80-91da-38e1b287c95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4970692-3d56-4d7a-8c20-9ca1afffbf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44f0e1-8748-486f-9915-07d37d6e86fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9e857-8f6a-4323-ad72-daf72fa2abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31334bd4-e148-491e-898e-8982f8c71bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447b232-240d-4d31-965b-21045e3b5fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDIL",
   "language": "python",
   "name": "sdil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

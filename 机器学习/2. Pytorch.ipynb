{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ce74fd",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "PyTorch是人工智能方面重要的工具之一，但是它的最本质功能就是自动求解梯度，因为求解梯度是绝大多数数值优化的重要组成部分。换言之，PyTorch不仅可以用于神经网络，而可以用于几乎所有的（具有梯度的）优化问题。下面我们就来看一个例子（本节以代码为主）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d18b0",
   "metadata": {},
   "source": [
    ">举例：还是接上文的例子，我们现在有数据\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\boldsymbol{x}_1=[0.2, 0.5, 0.7]^\\top &&\\boldsymbol{y}_1=[0.8, 0.8]^\\top\\\\\n",
    "&\\boldsymbol{x}_2=[0.4, 0.6, 0.8]^\\top &&\\boldsymbol{y}_2=[0.7, 0.9]^\\top\\\\\n",
    "&\\boldsymbol{x}_3=[0.3, 0.6, 0.9]^\\top &&\\boldsymbol{y}_3=[0.9, 0.9]^\\top\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92950d19",
   "metadata": {},
   "source": [
    "## 调用PyTorch包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0807e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65f119",
   "metadata": {},
   "source": [
    "## 输入数据\n",
    "绝大多数情况下数据不需要手动输入，它们往往会在数据采集的时候自动记录下来。这里我们用PyTorch自带的数据类型`torch.tensor`，这个类型也可以从其他数据类型转换过来。\n",
    "\n",
    "**强调：在机器学习里，多个训练数据往往拼接到一起，而不是逐个计算，并且数据的维度往往是$E\\times M$，也就是每一行是一个数据，每一列是一个特征。后面我们会把数据对应这个规则来输入。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b096ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.5000, 0.7000],\n",
       "        [0.4000, 0.6000, 0.8000],\n",
       "        [0.3000, 0.6000, 0.9000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([0.2, 0.5, 0.7, 0.4, 0.6, 0.8, 0.3, 0.6, 0.9]).view(3,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af5bc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 0.8000],\n",
       "        [0.7000, 0.9000],\n",
       "        [0.9000, 0.9000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([0.8, 0.8, 0.7, 0.9, 0.9, 0.9]).view(3,2)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac42955",
   "metadata": {},
   "source": [
    "## 定义参数\n",
    "如上一节所示，要优化参数，就需要计算参数的梯度。在PyTorch定义tensor的时候加上`requires_grad=True`即可。\n",
    "\n",
    "**由于数据维度的要求，我们让`W`是一个$M\\times N$的矩阵，并且让$\\boldsymbol{y}=\\boldsymbol{x}\\cdot W$，这样$\\boldsymbol{y}$的维度就是$E\\times M\\cdot M\\times N = E\\times N$了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf5e789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9407, -0.1843],\n",
       "        [-0.5164, -1.0036],\n",
       "        [ 0.7652,  0.0561]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn([3,2], requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c655dd",
   "metadata": {},
   "source": [
    "这里我们也可以看到`W`的最后标注了`requires_grad=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80ba2c",
   "metadata": {},
   "source": [
    "## 定义模型的计算过程\n",
    "$\\hat{\\boldsymbol{y}}=\\boldsymbol{x}\\cdot W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d7b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w, x):\n",
    "    return torch.matmul(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d833c",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "$\\mathcal{L}(\\hat{\\boldsymbol{y}}, \\boldsymbol{y})=\\|\\hat{\\boldsymbol{y}}- \\boldsymbol{y}\\|_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416b8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    return torch.norm(yhat-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04126f",
   "metadata": {},
   "source": [
    "## 利用PyTorch计算梯度\n",
    "PyTorch不能计算梯度的解析解，而是在每一个$W$处计算梯度。因此需要\n",
    "* 利用模型的参数`W`和数据`X`计算`y_hat`\n",
    "* 利用`y_hat`和数据`Y`计算损失函数`L`\n",
    "* 利用损失函数`L`计算`W`的梯度\n",
    "\n",
    "可以看出，这个过程可以大致分成2部分：\n",
    "* 前向传播，也就是从$x\\to \\hat{y} \\to \\mathcal{L}$\n",
    "* 反向传播，也就是$\\mathcal{L} \\to \\nabla_w$\n",
    "\n",
    "反向传播的代码是`L.backward()`，求解的就是$\\nabla_w\\mathcal{L}$\n",
    "\n",
    "查看参数的梯度使用的是`W.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1537b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2943, -2.6488],\n",
      "        [-0.6469, -4.9450],\n",
      "        [-0.9326, -6.9814]])\n"
     ]
    }
   ],
   "source": [
    "# 正向传播\n",
    "y_hat = f(W, X)\n",
    "L = loss(y_hat, Y)\n",
    "\n",
    "# 反向传播\n",
    "L.backward()\n",
    "\n",
    "# 显示梯度\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb14f8",
   "metadata": {},
   "source": [
    "**要注意的是，反向传播`.backward()`所产生的梯度会不断累积：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3977a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再次反向传播\n",
      "tensor([[ -0.5886,  -5.2975],\n",
      "        [ -1.2937,  -9.8900],\n",
      "        [ -1.8652, -13.9628]])\n",
      "再再次反向传播\n",
      "tensor([[ -0.8828,  -7.9463],\n",
      "        [ -1.9406, -14.8350],\n",
      "        [ -2.7978, -20.9441]])\n",
      "再再再次反向传播\n",
      "tensor([[ -1.1771, -10.5951],\n",
      "        [ -2.5875, -19.7801],\n",
      "        [ -3.7303, -27.9255]])\n"
     ]
    }
   ],
   "source": [
    "print('再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)\n",
    "\n",
    "print('再再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)\n",
    "\n",
    "print('再再再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11431807",
   "metadata": {},
   "source": [
    "这一点有利有弊。对于刚接触PyTorch的人来说，忘记及时清除已有梯度，导致梯度错误，并且不断累积。但是这个性质也使得许多操作成为可能（有些人可能不会接触到）。\n",
    "\n",
    "那么要清除现有梯度就需要`W.grad.zero_()`\n",
    "\n",
    "这样我们再来测试一次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4512eec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再次反向传播\n",
      "tensor([[ -1.4714, -13.2439],\n",
      "        [ -3.2344, -24.7251],\n",
      "        [ -4.6629, -34.9069]])\n",
      "再再次反向传播\n",
      "tensor([[-0.2943, -2.6488],\n",
      "        [-0.6469, -4.9450],\n",
      "        [-0.9326, -6.9814]])\n",
      "再再再次反向传播\n",
      "tensor([[-0.2943, -2.6488],\n",
      "        [-0.6469, -4.9450],\n",
      "        [-0.9326, -6.9814]])\n",
      "梯度不再累积\n"
     ]
    }
   ],
   "source": [
    "print('再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)\n",
    "W.grad.zero_()\n",
    "\n",
    "print('再再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)\n",
    "W.grad.zero_()\n",
    "\n",
    "print('再再再次反向传播')\n",
    "loss(f(W, X), Y).backward()\n",
    "print(W.grad)\n",
    "W.grad.zero_()\n",
    "\n",
    "print('梯度不再累积')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b830b",
   "metadata": {},
   "source": [
    "可以看到，这样梯度就不会累积了。这样我们就可以在每一个点计算新的梯度，然后更新参数。那么整个优化过程就变成了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a47ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代后的参数为：\n",
      "tensor([[ 2.5600,  0.8600],\n",
      "        [-0.7600,  0.0900],\n",
      "        [ 0.5500,  2.1400]])\n",
      "第3000次迭代后的参数为：\n",
      "tensor([[-1.3000, -0.2400],\n",
      "        [ 0.8100,  1.6400],\n",
      "        [ 0.9100,  0.0100]])\n",
      "第6000次迭代后的参数为：\n",
      "tensor([[-1.4400, -0.4300],\n",
      "        [ 1.3100,  2.2600],\n",
      "        [ 0.6200, -0.3600]])\n",
      "第9000次迭代后的参数为：\n",
      "tensor([[-1.4800, -0.4800],\n",
      "        [ 1.4400,  2.4300],\n",
      "        [ 0.5300, -0.4600]])\n",
      "第12000次迭代后的参数为：\n",
      "tensor([[-1.5000, -0.4900],\n",
      "        [ 1.4800,  2.4800],\n",
      "        [ 0.5100, -0.4900]])\n",
      "第15000次迭代后的参数为：\n",
      "tensor([[-1.5000, -0.5000],\n",
      "        [ 1.5000,  2.4900],\n",
      "        [ 0.5000, -0.5000]])\n",
      "第18000次迭代后的参数为：\n",
      "tensor([[-1.5000, -0.5000],\n",
      "        [ 1.5000,  2.5000],\n",
      "        [ 0.5000, -0.5000]])\n",
      "第21000次迭代后的参数为：\n",
      "tensor([[-1.5000, -0.5000],\n",
      "        [ 1.5000,  2.5000],\n",
      "        [ 0.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 输入/读取数据\n",
    "X = torch.tensor([0.2, 0.5, 0.7, 0.4, 0.6, 0.8, 0.3, 0.6, 0.9]).view(3,3)\n",
    "Y = torch.tensor([0.8, 0.8, 0.7, 0.9, 0.9, 0.9]).view(3,2)\n",
    "\n",
    "# 定义模型，损失函数，学习率\n",
    "W = torch.randn([3,2], requires_grad=True)\n",
    "def f(w, x):\n",
    "    return torch.matmul(x, w)\n",
    "def loss(yhat, y):\n",
    "    return torch.norm(yhat-y)**2\n",
    "alpha = 0.25\n",
    "\n",
    "# 优化\n",
    "for i in range(21001):\n",
    "    # 正向传播\n",
    "    y_hat = f(W, X)\n",
    "    L = loss(y_hat, Y)\n",
    "\n",
    "    # 反向传播\n",
    "    L.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    W.data = W.data - alpha * W.grad\n",
    "\n",
    "    # 重置梯度\n",
    "    W.grad.zero_()\n",
    "\n",
    "    # 显示参数变化    \n",
    "    if not i%3000:\n",
    "        print(f'第{i}次迭代后的参数为：\\n{torch.round(W.data*100)/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d6ffd",
   "metadata": {},
   "source": [
    "得到的结果和之前的结果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badc11c",
   "metadata": {},
   "source": [
    "## 总结\n",
    "从最后一个代码块可以看出，利用PyTorch，我们只需要定义前向传播的函数（模型）以及损失函数，梯度就可以自动被计算出来并且用于梯度下降，完全不需要任何公式推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f5844",
   "metadata": {},
   "source": [
    "# 讨论\n",
    "## 关于`torch.tensor`\n",
    "可以看出，在代码中`W`是一个`tensor`的数据结构，他主要包含2个部分\n",
    "* `W.data`，表示了它的数值\n",
    "* `W.grad`，储存了它的梯度\n",
    "也就是说，`tensor`可以看做是一个容器，它存了很多东西在里面。\n",
    "\n",
    "## 关于优化\n",
    "本章节只用到了`tensor.backward()`用来计算参数的梯度。其实PyTorch还提供大量的功能，例如定义模型、参数更新等功能。利用这些功能，代码可以更加简洁高效。后面会继续讲解。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2226728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

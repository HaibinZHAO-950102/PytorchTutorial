{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c1a0d5",
   "metadata": {},
   "source": [
    "# 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52f9de",
   "metadata": {},
   "source": [
    "一个函数的梯度就是它的值上升速度最快的方向，我们一般用$\\nabla_w f$表示函数$f$对$w$的梯度。从计算的角度来讲，一个函数的梯度就是它对每个变量的导数，例如\n",
    ">$f(\\boldsymbol{x})=x_1^2-\\log x_2$的对于$\\boldsymbol{x}$梯度就是\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x})=\n",
    "\\begin{bmatrix}\n",
    "2x_1\\\\-x_2^{-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "在任意一点，例如$\\boldsymbol{x}=[1,2]^\\top$处，梯度方向就是$[2,-1/2]^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f59b61",
   "metadata": {},
   "source": [
    "# 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfad7b",
   "metadata": {},
   "source": [
    "我们已经提到，训练机器学习的模型就是一个通过优化$\\boldsymbol{w}$来最小化loss function的过程，那么通过不断的向负梯度方向移动，我们就能不断减少目标函数的值，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451d631",
   "metadata": {},
   "source": [
    ">举例：接上文的例子，我们现在有数据\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\boldsymbol{x}_1=[0.2, 0.5, 0.7]^\\top &&\\boldsymbol{y}_1=[0.8, 0.8]^\\top\\\\\n",
    "&\\boldsymbol{x}_2=[0.4, 0.6, 0.8]^\\top &&\\boldsymbol{y}_2=[0.7, 0.9]^\\top\\\\\n",
    "&\\boldsymbol{x}_3=[0.3, 0.6, 0.9]^\\top &&\\boldsymbol{y}_3=[0.9, 0.9]^\\top\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "那么模型的输出就是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{y}}_1=W\\cdot\\boldsymbol{x}_1=\n",
    "\\begin{bmatrix}\n",
    "0.2 w_{11}+0.5w_{12}+0.7w_{13}\\\\\n",
    "0.2 w_{21}+0.5w_{22}+0.7w_{23}\n",
    "\\end{bmatrix}\\\\\n",
    "\\hat{\\boldsymbol{y}}_2=W\\cdot\\boldsymbol{x}_2=\n",
    "\\begin{bmatrix}\n",
    "0.4 w_{11}+0.6w_{12}+0.8w_{13}\\\\\n",
    "0.4 w_{21}+0.6w_{22}+0.8w_{23}\n",
    "\\end{bmatrix}\\\\\n",
    "\\hat{\\boldsymbol{y}}_3=W\\cdot\\boldsymbol{x}_3=\n",
    "\\begin{bmatrix}\n",
    "0.3 w_{11}+0.6w_{12}+0.9w_{13}\\\\\n",
    "0.3 w_{21}+0.6w_{22}+0.9w_{23}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "假设我们选择$\\ell_2$的平方为损失函数，那么\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_i\\mathcal{L}\\{\\hat{\\boldsymbol{y}},\\boldsymbol{y}\\}&=\\sum_i\\|\\hat{\\boldsymbol{y}}-\\boldsymbol{y}\\|_2^2\\\\\n",
    "&=\n",
    "\\left\\|\\begin{bmatrix}\n",
    "0.2 w_{11}+0.5w_{12}+0.7w_{13}-0.8\\\\\n",
    "0.2 w_{21}+0.5w_{22}+0.7w_{23}-0.8\n",
    "\\end{bmatrix}\\right\\|_2^2\\\\\n",
    "&+\n",
    "\\left\\|\\begin{bmatrix}\n",
    "0.4 w_{11}+0.6w_{12}+0.8w_{13}-0.7\\\\\n",
    "0.4 w_{21}+0.6w_{22}+0.8w_{23}-0.9\n",
    "\\end{bmatrix}\\right\\|_2^2\\\\\n",
    "&+\n",
    "\\left\\|\\begin{bmatrix}\n",
    "0.3 w_{11}+0.6w_{12}+0.9w_{13}-0.9\\\\\n",
    "0.3 w_{21}+0.6w_{22}+0.9w_{23}-0.9\n",
    "\\end{bmatrix}\\right\\|_2^2\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "实际上可以轻易的求出损失函数对于每个$w_{m,n}$的导数，例如\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}}\n",
    "&=0.2*2*(0.2 w_{11}+0.5w_{12}+0.7w_{13}-0.8)\\\\\n",
    "&+0.4*2*(0.4 w_{11}+0.6w_{12}+0.8w_{13}-0.7)\\\\\n",
    "&+0.3*2*(0.3 w_{11}+0.6w_{12}+0.9w_{13}-0.9)\\\\\n",
    "&=0.58 w_{11}+1.04w_{12}+1.46w_{13}-1.42\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{12}}\n",
    "&=0.5*2*(0.2 w_{11}+0.5w_{12}+0.7w_{13}-0.8)\\\\\n",
    "&+0.6*2*(0.4 w_{11}+0.6w_{12}+0.8w_{13}-0.7)\\\\\n",
    "&+0.6*2*(0.3 w_{11}+0.6w_{12}+0.9w_{13}-0.9)\\\\\n",
    "&=1.04 w_{11}+1.94w_{12}+2.74w_{13}-2.72\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{13}}\n",
    "&=0.7*2*(0.2 w_{11}+0.5w_{12}+0.7w_{13}-0.8)\\\\\n",
    "&+0.8*2*(0.4 w_{11}+0.6w_{12}+0.8w_{13}-0.7)\\\\\n",
    "&+0.9*2*(0.3 w_{11}+0.6w_{12}+0.9w_{13}-0.9)\\\\\n",
    "&=1.46 w_{11}+2.74w_{12}+3.88w_{13}-3.86\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{21}}\n",
    "&=0.2*2*(0.2 w_{21}+0.5w_{22}+0.7w_{23}-0.8)\\\\\n",
    "&+0.4*2*(0.4 w_{21}+0.6w_{22}+0.8w_{23}-0.9)\\\\\n",
    "&+0.3*2*(0.3 w_{21}+0.6w_{22}+0.9w_{23}-0.9)\\\\\n",
    "&=0.58 w_{21}+1.04w_{22}+1.46w_{23}-1.58\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{22}}\n",
    "&=0.5*2*(0.2 w_{21}+0.5w_{22}+0.7w_{23}-0.8)\\\\\n",
    "&+0.6*2*(0.4 w_{21}+0.6w_{22}+0.8w_{23}-0.9)\\\\\n",
    "&+0.6*2*(0.3 w_{21}+0.6w_{22}+0.9w_{23}-0.9)\\\\\n",
    "&=1.04 w_{21}+1.94w_{22}+2.74w_{23}-2.96\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{23}}\n",
    "&=0.7*2*(0.2 w_{21}+0.5w_{22}+0.7w_{23}-0.8)\\\\\n",
    "&+0.8*2*(0.4 w_{21}+0.6w_{22}+0.8w_{23}-0.9)\\\\\n",
    "&+0.9*2*(0.3 w_{21}+0.6w_{22}+0.9w_{23}-0.9)\\\\\n",
    "&=1.46 w_{21}+2.74w_{22}+3.88w_{23}-4.18\n",
    "\\end{aligned}\n",
    "$$\n",
    "也就是说，对于任意一点$\\boldsymbol{w}=[w_{11},...,w_{23}]^\\top$，损失函数的梯度方向就是\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{w}}\\mathcal{L}=\n",
    "\\begin{bmatrix}\n",
    "0.58 w_{11}+1.04w_{12}+1.46w_{13}-1.42\\\\\n",
    "1.04 w_{11}+1.94w_{12}+2.74w_{13}-2.72\\\\\n",
    "1.46 w_{11}+2.74w_{12}+3.88w_{13}-3.86\\\\\n",
    "0.58 w_{21}+1.04w_{22}+1.46w_{23}-1.58\\\\\n",
    "1.04 w_{21}+1.94w_{22}+2.74w_{23}-2.96\\\\\n",
    "1.46 w_{21}+2.74w_{22}+3.88w_{23}-4.18\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d2619",
   "metadata": {},
   "source": [
    "只要我们不断沿着负梯度方向迭代当前$\\boldsymbol{w}$，损失函数的值就会不断下降。因此，我们可以得到一个显然的结论，那就是优化过程会收敛于梯度为0的地方。\n",
    "\n",
    "\n",
    "由于例子中的问题过于简单，显然可以看出问题收敛于\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{w}}\\mathcal{L}=\n",
    "\\begin{bmatrix}\n",
    "0.58 w_{11}+1.04w_{12}+1.46w_{13}-1.42\\\\\n",
    "1.04 w_{11}+1.94w_{12}+2.74w_{13}-2.72\\\\\n",
    "1.46 w_{11}+2.74w_{12}+3.88w_{13}-3.86\\\\\n",
    "0.58 w_{21}+1.04w_{22}+1.46w_{23}-1.58\\\\\n",
    "1.04 w_{21}+1.94w_{22}+2.74w_{23}-2.96\\\\\n",
    "1.46 w_{21}+2.74w_{22}+3.88w_{23}-4.18\\\\\n",
    "\\end{bmatrix}=\\boldsymbol{0}\n",
    "$$\n",
    "这个问题有解析解，也就是\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.58 & 1.04 & 1.46 & 0 & 0 & 0\\\\\n",
    "1.04 & 1.94 & 2.74 & 0 & 0 & 0\\\\\n",
    "1.46 & 2.74 & 3.88 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0.58 & 1.04 & 1.46\\\\\n",
    "0 & 0 & 0 & 1.04 & 1.94 & 2.74\\\\\n",
    "0 & 0 & 0 & 1.46 & 2.74 & 3.88\\\\\n",
    "\\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{11}\\\\w_{12}\\\\w_{12}\\\\w_{21}\\\\w_{22}\\\\w_{23}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1.42\\\\\n",
    "2.72\\\\\n",
    "3.86\\\\\n",
    "1.58\\\\\n",
    "2.96\\\\\n",
    "4.18\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "可以轻易求出\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_{11}\\\\w_{12}\\\\w_{12}\\\\w_{21}\\\\w_{22}\\\\w_{23}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-1.5\\\\1.5\\\\0.5\\\\-0.5\\\\2.5\\\\-0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "或者变形为\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "-1.5&1.5&0.5\\\\-0.5&2.5&-0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "（下面提供解决这个解析解的计算代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "269513b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.5]\n",
      " [ 1.5]\n",
      " [ 0.5]\n",
      " [-0.5]\n",
      " [ 2.5]\n",
      " [-0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([0.58, 1.04, 1.46, 0, 0, 0, \n",
    "              1.04, 1.94, 2.74, 0, 0, 0,\n",
    "              1.46, 2.74, 3.88, 0, 0, 0,\n",
    "              0, 0, 0, 0.58, 1.04, 1.46,\n",
    "              0, 0, 0, 1.04, 1.94, 2.74,\n",
    "              0, 0, 0, 1.46, 2.74, 3.88]).reshape(6,6)\n",
    "\n",
    "y = np.array([1.42, 2.72, 3.86, 1.58, 2.96, 4.18]).reshape(-1,1)\n",
    "\n",
    "w = np.matmul(np.linalg.inv(A), y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb40f5",
   "metadata": {},
   "source": [
    "然而现实往往是残酷的：一般的机器学习（特别是深度学习）的模型不会是$f_{\\boldsymbol{w}}(\\boldsymbol{x})=W\\cdot\\boldsymbol{x}$这么直观，因此往往没有解析解，这时候就需要用到数值优化。最简单也是最常用的就是梯度下降法，这里最关键的就是3个因素：\n",
    "* 起始点$\\boldsymbol{w}$，一般是随机选取\n",
    "* 搜索方向，也就是负梯度方向（也有其他基于梯度的改进方案）\n",
    "* 步长，也叫学习率（往往用$\\alpha$或者`lr`表示，需要人为设置）\n",
    "* 停止条件，一般用early stop原则，后面会解释到。\n",
    "\n",
    "整个优化的过程就是：\n",
    "* 选取起始点$\\boldsymbol{w}$\n",
    "* 计算当前点的梯度$\\nabla$\n",
    "* 更新参数，也就是把参数向负梯度方向移动$\\alpha$的距离，也就是$w:=w-\\alpha\\cdot\\nabla$\n",
    "\n",
    "下面的代码体现了一个梯度优化的过程，这个例子中我们以迭代20次为停止条件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a43ac4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代后的参数为：\n",
      "[ 0.52  2.14  1.22  0.76 -0.9   1.84]\n",
      "第3000次迭代后的参数为：\n",
      "[-1.48  1.42  0.54 -0.2   1.49  0.1 ]\n",
      "第6000次迭代后的参数为：\n",
      "[-1.49  1.48  0.51 -0.41  2.21 -0.33]\n",
      "第9000次迭代后的参数为：\n",
      "[-1.5   1.49  0.5  -0.48  2.42 -0.45]\n",
      "第12000次迭代后的参数为：\n",
      "[-1.5   1.5   0.5  -0.49  2.48 -0.49]\n",
      "第15000次迭代后的参数为：\n",
      "[-1.5   1.5   0.5  -0.5   2.49 -0.5 ]\n",
      "第18000次迭代后的参数为：\n",
      "[-1.5  1.5  0.5 -0.5  2.5 -0.5]\n",
      "第21000次迭代后的参数为：\n",
      "[-1.5  1.5  0.5 -0.5  2.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 选取随机起始点\n",
    "w = np.random.randn(6).reshape(-1,1)\n",
    "# 选取学习率\n",
    "alpha = 0.25\n",
    "\n",
    "# 用于计算梯度的矩阵\n",
    "A = np.array([0.58, 1.04, 1.46, 0, 0, 0, \n",
    "              1.04, 1.94, 2.74, 0, 0, 0,\n",
    "              1.46, 2.74, 3.88, 0, 0, 0,\n",
    "              0, 0, 0, 0.58, 1.04, 1.46,\n",
    "              0, 0, 0, 1.04, 1.94, 2.74,\n",
    "              0, 0, 0, 1.46, 2.74, 3.88]).reshape(6,6)\n",
    "y = np.array([1.42, 2.72, 3.86, 1.58, 2.96, 4.18]).reshape(-1,1)\n",
    "\n",
    "for i in range(21001):\n",
    "    # 计算梯度\n",
    "    nabla = np.matmul(A, w)-y\n",
    "    # 更新参数\n",
    "    w = w - alpha * nabla\n",
    "\n",
    "    # 显示参数变化    \n",
    "    if not i%3000:\n",
    "        print(f'第{i}次迭代后的参数为：\\n{np.round(w.flatten(),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb39da",
   "metadata": {},
   "source": [
    "可以看出，通过数值方法，我们也能得到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673c0a3",
   "metadata": {},
   "source": [
    "# 补充\n",
    "梯度下降是深度学习中最具主导地位的优化方法。我们可以看出，这个梯度下降法的过程中最复杂的部分就是推导梯度，然而这个过程可以被许多工具自动化进行（事实上，除了梯度计算之外的很多更多其他功能也已经被实现），比如PyTorch, TensorFlow等等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
